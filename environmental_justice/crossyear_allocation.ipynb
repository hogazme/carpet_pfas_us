{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import LineString\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import contextily as ctx\n",
    "from numpy import int64\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_tracts10 = gpd.read_file('assignment/tiger/tiger_tracts_2010.geojson')\n",
    "tiger_tracts10 = tiger_tracts10[['GEOID10', 'geometry']]\n",
    "tiger_tracts10.drop_duplicates(subset=['GEOID10'], inplace=True)\n",
    "tiger_tracts10.reset_index(inplace=True, drop=True)\n",
    "tiger_tracts10.rename(columns={'GEOID10': 'GEOID'}, inplace=True)\n",
    "\n",
    "tiger_tracts20 = gpd.read_file('assignment/tiger/tiger_tracts_2020.geojson')\n",
    "tiger_tracts20 = tiger_tracts20[['GEOID', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# population assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_tracts20 = gpd.read_file('tiger_tracts_2020.geojson')\n",
    "tiger_tracts20 = tiger_tracts20[['GEOID', 'geometry']]\n",
    "tiger_tracts20['GEOID'] = tiger_tracts20['GEOID'].astype(int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import int64\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def landfill_input_file(year):\n",
    "    landfill = pd.read_csv('lanfill_FINAL.csv')\n",
    "    before = landfill[landfill['Year Landfill Opened'] > year]\n",
    "    landfill = landfill.drop(before.index)\n",
    "    landfill = landfill.dropna(subset=['Latitude', 'Longitude'])\n",
    "    landfill.drop_duplicates(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "    landfill.reset_index(inplace=True, drop=True)\n",
    "    landfill_gdf = gpd.GeoDataFrame(landfill, geometry=gpd.points_from_xy(landfill.Longitude, landfill.Latitude), crs='EPSG:4269')\n",
    "    landfill_gdf['facility_x'] = landfill_gdf.geometry.x\n",
    "    landfill_gdf['facility_y'] = landfill_gdf.geometry.y\n",
    "    landfill_gdf.rename(columns={'Landfill ID': 'facility_id'}, inplace=True)\n",
    "    landfill_gdf_sub = landfill_gdf[['facility_id', 'facility_x', 'facility_y']]\n",
    "    return landfill_gdf_sub\n",
    "\n",
    "def landfill_input_file_heatmap(year):\n",
    "    landfill = pd.read_csv('lanfill_FINAL.csv')\n",
    "    before = landfill[landfill['Year Landfill Opened'] <= year]\n",
    "    landfill = landfill.drop(before.index)\n",
    "    landfill = landfill.dropna(subset=['Latitude', 'Longitude'])\n",
    "    landfill.drop_duplicates(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "    landfill.reset_index(inplace=True, drop=True)\n",
    "    landfill_gdf = gpd.GeoDataFrame(landfill, geometry=gpd.points_from_xy(landfill.Longitude, landfill.Latitude), crs='EPSG:4269')\n",
    "    landfill_gdf['facility_x'] = landfill_gdf.geometry.x\n",
    "    landfill_gdf['facility_y'] = landfill_gdf.geometry.y\n",
    "    landfill_gdf.rename(columns={'Landfill ID': 'facility_id'}, inplace=True)\n",
    "    landfill_gdf_sub = landfill_gdf[['facility_id', 'facility_x', 'facility_y']]\n",
    "    return landfill_gdf_sub\n",
    "\n",
    "def add_impacted_column(sample_facilities, sample_pfas, dist_threshold_miles):\n",
    "    facilities_coords = list(zip(sample_facilities['facility_y'], sample_facilities['facility_x']))\n",
    "    pfas_coords = list(zip(sample_pfas['ct_y'], sample_pfas['ct_x']))\n",
    "\n",
    "    facility_tree = cKDTree(facilities_coords)\n",
    "    nearby_facility_ids = []\n",
    "\n",
    "    for pfas_point in pfas_coords:\n",
    "        distances, indices = facility_tree.query(pfas_point, k=15)\n",
    "        if distances[0] <= dist_threshold_miles:\n",
    "            within_threshold = [sample_facilities.iloc[i]['facility_id'] for i in indices if geodesic(pfas_point, facilities_coords[i]).km <= dist_threshold_miles]\n",
    "            nearby_facility_ids.append(within_threshold)\n",
    "        else:\n",
    "            nearby_facility_ids.append([])\n",
    "    sample_pfas['nearby_facility_ids'] = nearby_facility_ids\n",
    "    sample_pfas['impacted'] = sample_pfas['nearby_facility_ids'].apply(lambda x: len(x) > 0)\n",
    "    return sample_pfas\n",
    "\n",
    "def get_impacted_populations(year, dist_threshold):\n",
    "    base_scenario = '01_CTAccuYear_High_StatusQuo'\n",
    "    sample_pfas = pd.read_csv('Allocation to Census tracts_v3/Allocation to Census tracts_v3/{}_results/{}_{}_pfas_all_assigned.csv'.format(base_scenario[3:], base_scenario[3:], year), usecols=['GEOID', 'ct_x', 'ct_y'])\n",
    "    sample_facilities = landfill_input_file(year)\n",
    "\n",
    "    ct_pop = pd.read_csv(\"Census tracts' Population/Census tracts' Population/CT_Population_{}.csv\".format(year))\n",
    "    ct_pop['GEOID'] = ct_pop['GEO_ID'].str[9:]\n",
    "    ct_pop['GEOID'] = ct_pop['GEOID'].astype(int64)\n",
    "    sample_pfas = sample_pfas.merge(ct_pop, on='GEOID', how='left')\n",
    "    sample_pfas = add_impacted_column(sample_facilities, sample_pfas, dist_threshold)\n",
    "    \n",
    "    # sample_pfas.to_csv('Allocation to Census tracts_v3/population_results/{}_{}km_pfas_all_assigned.csv'.format(year, dist_threshold), index=False)\n",
    "    return print('Done processing {} {}.'.format(year, dist_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing 2020 1.\n",
      "Done processing 2020 3.\n",
      "Done processing 2020 5.\n",
      "Done processing 2060 1.\n",
      "Done processing 2060 3.\n",
      "Done processing 2060 5.\n"
     ]
    }
   ],
   "source": [
    "dist_thresholds = [1, 3, 5]\n",
    "years = [2020, 2060]\n",
    "\n",
    "for year in years:\n",
    "    for dist_threshold in dist_thresholds:\n",
    "        get_impacted_populations(year, dist_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impacted_populations_annual(year, dist_threshold):\n",
    "    base_scenario = '01_CTAccuYear_High_StatusQuo'\n",
    "    sample_pfas = pd.read_csv('Allocation to Census tracts_v3/Allocation to Census tracts_v3/{}_results/{}_{}_pfas_all_assigned.csv'.format(base_scenario[3:], base_scenario[3:], year), usecols=['GEOID', 'ct_x', 'ct_y'])\n",
    "    sample_facilities = landfill_input_file_heatmap(year)\n",
    "    # sample_facilities = pd.read_csv('Allocation to Census tracts_v3/Allocation to Census tracts_v3/{}_results/{}_{}_MLF_pfas_assigned.csv'.format(base_scenario[3:], base_scenario[3:], year), usecols=['facility_id', 'facility_x', 'facility_y'])\n",
    "    if year<2010:\n",
    "        ct_pop = pd.read_csv(\"Census tracts' Population/Census tracts' Population/CT_Population_1990-2009.csv\".format(year))\n",
    "    elif year>2020:\n",
    "        ct_pop = pd.read_csv(\"Census tracts' Population/Census tracts' Population/CT_Population_2022-2060.csv\".format(year))\n",
    "    else:\n",
    "        ct_pop = pd.read_csv(\"Census tracts' Population/Census tracts' Population/CT_Population_{}.csv\".format(year))\n",
    "    ct_pop['GEOID'] = ct_pop['GEO_ID'].str[9:]\n",
    "    ct_pop['GEOID'] = ct_pop['GEOID'].astype(int64)\n",
    "    sample_pfas = sample_pfas.merge(ct_pop, on='GEOID', how='left')\n",
    "    sample_pfas = add_impacted_column(sample_facilities, sample_pfas, dist_threshold)\n",
    "    \n",
    "    sample_pfas.to_csv('Allocation to Census tracts_v3/population_results/heatmaps/{}_{}km_pfas_all_assigned.csv'.format(year, dist_threshold), index=False)\n",
    "    return print('Done processing {} {}.'.format(year, dist_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing 2020 1.\n",
      "Done processing 2030 1.\n",
      "Done processing 2040 1.\n",
      "Done processing 2050 1.\n",
      "Done processing 2060 1.\n",
      "Done processing 2020 2.\n",
      "Done processing 2030 2.\n",
      "Done processing 2040 2.\n",
      "Done processing 2050 2.\n",
      "Done processing 2060 2.\n",
      "Done processing 2020 3.\n",
      "Done processing 2030 3.\n",
      "Done processing 2040 3.\n",
      "Done processing 2050 3.\n",
      "Done processing 2060 3.\n",
      "Done processing 2020 4.\n",
      "Done processing 2030 4.\n",
      "Done processing 2040 4.\n",
      "Done processing 2050 4.\n",
      "Done processing 2060 4.\n",
      "Done processing 2020 5.\n",
      "Done processing 2030 5.\n",
      "Done processing 2040 5.\n",
      "Done processing 2050 5.\n",
      "Done processing 2060 5.\n",
      "Done processing 2020 6.\n",
      "Done processing 2030 6.\n",
      "Done processing 2040 6.\n",
      "Done processing 2050 6.\n",
      "Done processing 2060 6.\n",
      "Done processing 2020 7.\n",
      "Done processing 2030 7.\n",
      "Done processing 2040 7.\n",
      "Done processing 2050 7.\n",
      "Done processing 2060 7.\n",
      "Done processing 2020 8.\n",
      "Done processing 2030 8.\n",
      "Done processing 2040 8.\n",
      "Done processing 2050 8.\n",
      "Done processing 2060 8.\n",
      "Done processing 2020 9.\n",
      "Done processing 2030 9.\n",
      "Done processing 2040 9.\n",
      "Done processing 2050 9.\n",
      "Done processing 2060 9.\n",
      "Done processing 2020 10.\n",
      "Done processing 2030 10.\n",
      "Done processing 2040 10.\n",
      "Done processing 2050 10.\n",
      "Done processing 2060 10.\n"
     ]
    }
   ],
   "source": [
    "dists = [i for i in range(1, 11)]\n",
    "for dist in dists:\n",
    "    # get_impacted_populations_annual(1990, dist)\n",
    "    # get_impacted_populations_annual(2000, dist)\n",
    "    # get_impacted_populations_annual(2010, dist)\n",
    "    get_impacted_populations_annual(2020, dist)\n",
    "    get_impacted_populations_annual(2030, dist)\n",
    "    get_impacted_populations_annual(2040, dist)\n",
    "    get_impacted_populations_annual(2050, dist)\n",
    "    get_impacted_populations_annual(2060, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "landfill_accu = pd.read_csv('Allocation to Census tracts_v3\\Re_ data and sample maps\\landfill_Accum_High_StatusQuo_WO_Leachate.csv')\n",
    "landfill_leachate = pd.read_csv('Allocation to Census tracts_v3\\Re_ data and sample maps\\LFLeachate_High_StatusQuo.csv')\n",
    "ejscreen = pd.read_csv('EJSCREEN/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI_csv/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI.csv')\n",
    "\n",
    "year = 2020\n",
    "dists = [i for i in range(1, 11)]\n",
    "for dist in dists:\n",
    "    sample_f = pd.read_csv('Allocation to Census tracts_v3/population_results/heatmaps/{}_{}km_pfas_all_assigned.csv'.format(year, dist))\n",
    "    sample_f['nearby_facility_ids'] = sample_f['nearby_facility_ids'].apply(ast.literal_eval)\n",
    "    sample_f = sample_f[sample_f['impacted'] == True].explode('nearby_facility_ids')[['GEOID', 'ct_x', 'ct_y', 'nearby_facility_ids']]\n",
    "    sample_f.reset_index(inplace=True, drop=True)\n",
    "    sample_f['nearby_facility_ids'] = sample_f['nearby_facility_ids'].astype(str)\n",
    "    sample_f.rename(columns={'nearby_facility_ids': 'facility_id'}, inplace=True)\n",
    "    sample_f_accu = sample_f.merge(landfill_accu[['facility_id', '{}'.format(year)]], on='facility_id', how='left')\n",
    "    sample_f_accu = sample_f_accu[['GEOID', '{}'.format(year)]].groupby('GEOID').sum().reset_index()\n",
    "    sample_f_accu.rename(columns={'{}'.format(year): 'accu_pfas_within_{}'.format(dist)}, inplace=True)\n",
    "    sample_f_leachate = sample_f.merge(landfill_leachate[['facility_id', '{}'.format(year)]], on='facility_id', how='left')\n",
    "    sample_f_leachate = sample_f_leachate[['GEOID', '{}'.format(year)]].groupby('GEOID').sum().reset_index()\n",
    "    sample_f_leachate.rename(columns={'{}'.format(year): 'leachate_pfas_within_{}'.format(dist)}, inplace=True)\n",
    "\n",
    "ejscreen.dropna(subset=['GEOID'], inplace=True)\n",
    "impacted_gids = ['accu_pfas_within_{}'.format(i) for i in range(1, 11)] + ['leachate_pfas_within_{}'.format(i) for i in range(1, 11)]\n",
    "impacted_geoids = ejscreen[ejscreen[impacted_gids].sum(axis=1) > 0]['GEOID'].values\n",
    "ejscreen = ejscreen[ejscreen['GEOID'].isin(impacted_geoids.tolist())]\n",
    "ejscreen.reset_index(inplace=True, drop=True)\n",
    "# ejscreen.to_csv('Allocation to Census tracts_v3/population_results/heatmaps/ejscreen_{}_impacted_cts.csv'.format(year), index=False)\n",
    "# ejscreen.to_csv('Allocation to Census tracts_v3/population_results/heatmaps/ejscreen_{}_impacted_cts_km.csv'.format(year), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ejscreen['accu_pfas_within_10'].quantile(np.arange(0, 1.1, 0.1))\n",
    "ejscreen['quantile_range'] = pd.cut(ejscreen['accu_pfas_within_10'], bins=quantiles, labels=False, include_lowest=True)\n",
    "ejscreen_og = pd.read_csv('EJSCREEN/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI_csv/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI.csv')\n",
    "\n",
    "def hmap(fcol, fcol_label):\n",
    "    heatmap_data = np.zeros((10, 10))  # 10x10 matrix for the heatmap\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Filter rows in the j-th quantile\n",
    "            quantile_rows = ejscreen[ejscreen['quantile_range'] == j]\n",
    "            quantile_rows= quantile_rows[quantile_rows[f'accu_pfas_within_{i+1}'] > 0]\n",
    "            # Calculate median income for the i-th pfas_within column within the j-th quantile\n",
    "            median_income = quantile_rows[fcol].median()\n",
    "            heatmap_data[j, i] = median_income\n",
    "\n",
    "    center_val = ejscreen_og[fcol].median()\n",
    "    ytlabels = ['[{:.1f}, {:.1f}]'.format(quantiles.tolist()[i], quantiles.tolist()[i+1]) for i in range(10)]\n",
    "    ytlabels = ytlabels[::-1]\n",
    "    sns.set(style='white', font_scale=1.5)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    heatmap_plot = sns.heatmap(heatmap_data[::-1], annot=True, fmt=\".1f\", cmap='RdYlGn_r', \n",
    "                # add the cells boundaries color to be white\n",
    "                linecolor='white', linewidth=0.5,\n",
    "                xticklabels=[f'{i}' for i in range(1, 11)], \n",
    "                # use quantiles list to label the y-axis\n",
    "                yticklabels=ytlabels, \n",
    "                # vmin=0, \n",
    "                # vmax=100\n",
    "                center=center_val\n",
    "                )\n",
    "    # plt.title('Median Income by PFAS Distance and Quantile')\n",
    "    colorbar = heatmap_plot.collections[0].colorbar\n",
    "    colorbar.set_label('Median {}'.format(fcol_label))\n",
    "    plt.xlabel('Exposure Threshold (Miles)')\n",
    "    plt.ylabel('Cumulated PFAS Range (kg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    " 'P_PEOPCOLORPCT',\n",
    " 'P_LOWINCPCT',\n",
    " 'P_UNEMPPCT',\n",
    " 'P_RSEI_AIR',\n",
    " 'P_D2_PM25',\n",
    "#  'P_D2_OZONE',\n",
    " 'P_D2_DSLPM',\n",
    "#  'P_D2_CANCER',\n",
    " 'P_D2_RESP',\n",
    "#  'P_D2_RSEI_AIR',\n",
    " 'P_D2_PTRAF',\n",
    " 'P_D2_LDPNT',\n",
    " 'P_D2_PNPL',\n",
    " 'P_D2_PRMP',\n",
    "#  'P_D2_PTSDF',\n",
    "#  'P_D2_UST',\n",
    " 'P_D2_PWDIS',\n",
    " ]\n",
    "metric_vars = [\n",
    " 'People of Color (%)',\n",
    " 'Low Income (%)',\n",
    " 'Unemployment (%)',\n",
    " 'Toxic Releases to Air',\n",
    " 'PM2.5',\n",
    "#  'P_D2_OZONE',\n",
    " 'Diesel PM',\n",
    "#  'P_D2_CANCER',\n",
    " 'Air Toxics Respiratory',\n",
    "#  'P_D2_RSEI_AIR',\n",
    " 'Traffic Proximity',\n",
    " 'Lead Paint',\n",
    " 'Superfund Proximity',\n",
    " 'RMP Proximity',\n",
    "#  'P_D2_PTSDF',\n",
    "#  'P_D2_UST',\n",
    " 'Wastewater Discharges',\n",
    " ]\n",
    "# feature_cols = [f + '_y' for f in feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scenario = '01_CTAccuYear_High_StatusQuo'\n",
    "\n",
    "def ej_impacted_process(year):\n",
    "    socio = gpd.read_file('tracts_w_pfas.geojson')\n",
    "    dists = [i for i in range(1, 11)]\n",
    "    impacted_gids = []\n",
    "    for dist in dists:\n",
    "        ipop_df = pd.read_csv('Allocation to Census tracts_v3/population_results/heatmaps/{}_{}km_pfas_all_assigned.csv'.format(year, dist))\n",
    "        mlf_df = pd.read_csv('Allocation to Census tracts_v3/Allocation to Census tracts_v3/{}_results/{}_{}_MLF_pfas_assigned.csv'.format(base_scenario[3:], base_scenario[3:], year))\n",
    "        wts_df = pd.read_csv('Allocation to Census tracts_v3/Allocation to Census tracts_v3/{}_results/{}_{}_WTS_pfas_assigned.csv'.format(base_scenario[3:], base_scenario[3:], year))\n",
    "\n",
    "        ipop_df['nearby_facility_ids'] = ipop_df['nearby_facility_ids'].apply(ast.literal_eval)\n",
    "        landfills = pd.concat([mlf_df[wts_df.columns], wts_df], axis=0)\n",
    "        landfills.reset_index(inplace=True, drop=True)\n",
    "        landfills['facility_id'] = landfills['facility_id'].astype(str)\n",
    "        # landfills.drop_duplicates(inplace=True)\n",
    "\n",
    "        ipop_df = ipop_df[ipop_df['impacted'] == True].explode('nearby_facility_ids')[['GEOID', 'ct_x', 'ct_y', 'nearby_facility_ids']]\n",
    "        ipop_df.reset_index(inplace=True, drop=True)\n",
    "        ipop_df['nearby_facility_ids'] = ipop_df['nearby_facility_ids'].astype(str)\n",
    "\n",
    "        merged_df = ipop_df.merge(landfills, left_on='nearby_facility_ids', right_on='facility_id', how='left')\n",
    "        merged_df.dropna(inplace=True)\n",
    "        merged_df = merged_df[['GEOID', '{}'.format(year)]].groupby('GEOID')['{}'.format(year)].sum().reset_index()\n",
    "        merged_df['GEOID'] = merged_df['GEOID'].astype(int64)\n",
    "        merged_df.rename(columns={'{}'.format(year): 'pfas_within_{}'.format(dist)}, inplace=True)\n",
    "        impacted_gids.append(merged_df['GEOID'].tolist())\n",
    "        socio = socio.merge(merged_df, on='GEOID', how='left')\n",
    "\n",
    "    ejscreen = pd.read_csv('D:/Users/hgazmeh/Codes/Datasets/EJSCREEN/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI_csv/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI.csv')\n",
    "    ejscreen = ejscreen.merge(socio, left_on='ID', right_on='GEOID', how='right')\n",
    "    for i in range(1, 11):\n",
    "        ejscreen['pfas_within_{}'.format(i)] = ejscreen['pfas_within_{}'.format(i)].fillna(0)\n",
    "    return ejscreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1990...\n",
      "Processing 2000...\n",
      "Processing 2010...\n",
      "Processing 2020...\n",
      "Processing 2030...\n",
      "Processing 2040...\n",
      "Processing 2050...\n",
      "Processing 2060...\n"
     ]
    }
   ],
   "source": [
    "for year in [1990, 2000, 2010, 2020, 2030, 2040, 2050, 2060]: # [1990, 2000, 2010, ]\n",
    "    print('Processing {}...'.format(year))\n",
    "    impacted_file = ej_impacted_process(year)\n",
    "    # impacted_file.to_csv('Allocation to Census tracts_v3/population_results/heatmaps/ejscreen_impacted_{}_full.csv'.format(year), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
